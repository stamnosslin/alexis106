<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Mats E. Nilsson" />

<meta name="date" content="2017-04-08" />

<title>ALEXIS 106: Method</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">ALEXIS 106: Method</h1>
<h4 class="author"><em>Mats E. Nilsson</em></h4>
<h4 class="date"><em>2017-04-08</em></h4>



<p>This packages contains data from an experiment on auditory abilities in blind and sighted listeners. The experiment was called ALEXIS 106, and the data set is called <code>data106</code>.</p>
<div id="background-and-aim" class="section level2">
<h2>Background and aim</h2>
<p>Many blind individuals use information available in sound reflections to detect and localize objects, so called echolocation. Echolocation may involve self-generated vocalizations, e.g., click sounds, or sounds generated by external sources in the environment, such as the sound from a car passing by.</p>
<p>Several studies have shown that blind listeners typically perform better than sighted listeners on echolocation tasks. This is presumably the results of many years of experience with navigating the environment without the aid of visual input. However, sighted listeners without any experience of echolocation can fairly quickly learn to perform well on specific echolocation tasks in laboratory settings.</p>
<p>The aim of this experiment was to compare blind and sighted listeners performance on four basic auditory tasks potentially related to human echolocation, to determine to what extent, of any, blind would perform better. The four abilities relates to changes in the acoustic signal as a direct sound is combined with its reflection from a nearby object.</p>
<ol style="list-style-type: decimal">
<li><p>The time-averaged sound pressure level (SPL) of the combined sound will be greater than the SPL of the direct sound. Thus, <em>intensity discrimination</em> is an ability that may be potentially useful for echolocation.</p></li>
<li><p>The direct and reflected sound will be correlated, resulting in a peak in the auto-correlation function at a position corresponding to the time-separation between direct and reflected sound. The amplitude of this peak relates to the relative amplitude of direct and reflected sound. Thus, the ability to detect weak indications of auto-correlation may potentially be useful for echolocation. This ability is here measured in what is called a <em>time-separation detection</em> task.</p></li>
<li><p>As the distance between listener and reflecting object changes, the location of the peak in the auto-correlation function will change. Thus a change in the auto-correlation function may provide information on distance to the reflecting object and thus may be an aspect relevant for echolocation. This relates to the ability to discriminate differences in time-separation between sounds combined of direct and reflected sound. This ability is here measured in what is called a <em>time-separation discrimination</em> task.</p></li>
<li><p>The spectral composition of the combined sound may differ from the spectral composition of the direct sound. This is true in particular for small reflecting objects that will function as low-pass filters because sound wavelengths greater than the size of the object will be diffracted around the object. Thus, the ability to detect small relative increments in the high-frequency part of the spectrum may be useful for echolocation. This ability is here measured in what is called a <em>high-frequency detection</em> task.</p></li>
</ol>
</div>
<div id="method" class="section level2">
<h2>Method</h2>
<div id="stimuli" class="section level3">
<h3>Stimuli</h3>
<p>The experiment included four experimental conditions, each containing one constant standard stimulus and one variable target stimulus, both 0.4 s long.</p>
<ol style="list-style-type: decimal">
<li><p><em>Intensity discrimination</em>. The standard stimulus was a white noise. The variable stimulus was the same white noise with a variable sound pressure level (always higher than the standard). Perceptually, the difference between standard and variable was a difference in loudness. Threshold data for this condition are given in the variable <code>th.1</code>, unit decibel re standard.</p></li>
<li><p><em>Time-separation detection</em>. The standard stimulus was a white noise. The variable stimulus was composed of the standard noise and a copy of the standard noise added to itself with a time-delay of 2 ms. The level of the added copy varied from equal to the standard to a lower level, depending on the listener’s performance. The overall level of the variable sound was always set equal to the standard sound to keep standard and variable equal in loudness. Perceptually, the difference between standard and variable was a difference in sound character. Threshold data for this condition are given in the variable <code>th.2</code>, unit decibel re standard.</p></li>
<li><p><em>Time-separation discrimination</em>. The standard stimulus was a white noise added to itself with a 2 ms delay (non-overlapping part of the stimulus not included). The variable stimulus was the same white noise added to itself with a variable time-separation. The overall level of the variable sound was always set equal to the standard sound to keep standard and variable equal in loudness. Perceptually, the difference between standard and variable was a difference in sound character. Threshold data for this condition are given in the variable <code>th.3</code>, unit samples (sampling frequency was 96 kHz, so 1 sample = 1/96000 s).</p></li>
<li><p><em>High-frequency detection</em>. The standard stimulus was a white noise. The variable stimulus was composed of the standard noise combined with a high-pass filtered white noise (cut-off frequency 4 kHz). The level of the added high-frequency noise varied from equal to the standard to a lower level, depending on the listener’s performance. The overall level of the variable sound was always set equal to the standard sound to keep standard and variable equal in loudness. Perceptually, the difference between standard and variable was a difference in sound character. Threshold data for this condition are given in the variable <code>th.4</code>, unit decibel re standard.</p></li>
</ol>
</div>
<div id="staircase-procedure" class="section level3">
<h3>Staircase procedure</h3>
<p>Thresholds were estimated using a three-interval, two-alternative forced choice task with an adaptive two-down, one-up rule that tracks the listener’s 71 % discrimination threshold. On the first interval, the standard was always presented. The second and third interval contained the standard and the variable in random order. The listeners task was to decide whether the second or the third interval contained a sound that differed from the first interval. This method is sometimes called “two-alternative forced choice with reminder”, where the first interval serves as the reminder. This method was used because it is hard to specify what perceptual characteristic that differs between standard and variable (with exception for the intensity discrimination task where the sound differ in loudness).</p>
<p>The staircase procedure started with an easily perceived difference between standard and variable. Two successive correct responses led to a reduction of the variable measure of 37 % (a step size of log 0.2 units) until the fourth reversal and by 11 % (0.05 log units) thereafter. An incorrect response led to an increase by the step size, or to the starting value if the rule implied an exceedance of this value. The starting values were: (1) 10 dB for the intensity discrimination task, (2) 0 dB for the time-separation detection task, (3) 8 ms for the time-separation discrimination task, and (4) 0 dB for the High-frequency detection task.</p>
</div>
<div id="threshold-estimates" class="section level3">
<h3>Threshold estimates</h3>
<p>Five runs were conducted for each of the six stimulus conditions in a random order unique to each listener. For each run, the threshold was estimated as the geometric mean at reversal points, after the fourth reversal. The geometric mean of the five runs for each condition was used as the estimate of the listener’s threshold for that condition.</p>
</div>
<div id="listeners" class="section level3">
<h3>Listeners</h3>
<p>Three groups of listeners were tested: 26 blind listeners, 26 sighted listeners, age-matched to the blind listeners, and 40 young listeners. These groups are referred to as “blind”, “matched” and “young” in the data set, variable <code>group</code>. Data on age, sex, and hearing status are available in the data set. For the blind listeners, there are also information on age at blindness onset, and self-reported use of echolocation</p>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
